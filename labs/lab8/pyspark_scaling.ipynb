{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn-lezQxWDka",
        "outputId": "e2112fb7-bba1-49ef-91a8-f4b15d64d46e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUINHu6iV4Mp"
      },
      "source": [
        "# Creating Your First Spark Program\n",
        "Import the SparkSession module from pyspark.sql and build a SparkSession with the `builder()` method.\n",
        "Afterwards, you can set the master URL to connect to, the application name, add some additional configuration\n",
        "like the executor memory and then lastly, use `getOrCreate()` to either get the current Spark session or to create\n",
        "one if there is none running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QHF0nwhoV4Mq"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ai1lRxk_V4Ms"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "# Import `DenseVector`\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "# Import `StandardScaler`\n",
        "from pyspark.ml.feature import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "24trFR3LV4Ms"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "   .master(\"local\") \\\n",
        "   .appName(\"Linear Regression Model\") \\\n",
        "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
        "   .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rq-wtMs2V4Mt"
      },
      "outputs": [],
      "source": [
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76aNzTCUV4Mt"
      },
      "source": [
        "This tutorial makes use of the California Housing data set. It appeared in a 1997 paper titled Sparse Spatial\n",
        "Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability\n",
        "Letters journal. The researchers built this data set by using the 1990 California census data.\n",
        "\n",
        "The data contains one row per census block group. A block group is the smallest geographical unit for which\n",
        "the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
        "In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
        "You’ll gather this information from this [web page](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
        "or by reading the paper which was mentioned above and which you can find [here](http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/html/ms_sp_lets1.html).\n",
        "\n",
        "These spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
        "\n",
        "* Longitude refers to the angular distance of a geographic place north or south of the earth’s equator for each block group;\n",
        "\n",
        "* Latitude refers to the angular distance of a geographic place east or west of the earth’s equator for each block group;\n",
        "\n",
        "* Housing median age is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values;\n",
        "\n",
        "* Total rooms is the total number of rooms in the houses per block group;\n",
        "\n",
        "* Total bedrooms is the total number of bedrooms in the houses per block group;\n",
        "\n",
        "* Population is the number of inhabitants of a block group;\n",
        "\n",
        "* Households refers to units of houses and their occupants per block group;\n",
        "\n",
        "* Median income is used to register the median income of people that belong to a block group; And,\n",
        "\n",
        "* Median house value is the dependent variable and refers to the median house value per block group.\n",
        "\n",
        "* What’s more, you also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n",
        "\n",
        "* The Median house value is the dependent variable and will be assigned the role of the target variable in your ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUyPbIAOV4Mu"
      },
      "source": [
        "\n",
        "Next, you’ll use the ```textFile()``` method to read in the data from the folder that you downloaded it to RDDs.\n",
        "This method takes an URI for the file, which is in this case the local path of your machine, and reads\n",
        "it as a collection of lines. For all convenience, you’ll not only read in the .data file, but also\n",
        "the .domain file that contains the header. This will allow you to double check the order of the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SojJ7hAnV4Mu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "cal_housing_data_url = 'https://drive.google.com/uc?export=download&id=1RaX_ymknCzYKcRIZB7fZ95bCyvoc9LST'\n",
        "cal_housing_domain_url = 'https://drive.google.com/uc?export=download&id=1R30F-YHKLcCNuaAGG2TJ9AgOLED3cdiK'\n",
        "\n",
        "r = requests.get(cal_housing_data_url)\n",
        "with open('cal_housing.data', 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "r = requests.get(cal_housing_domain_url)\n",
        "with open('cal_housing.domain', 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "rdd = sc.textFile('cal_housing.data')\n",
        "header = sc.textFile('cal_housing.domain')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ofsN0C7V4Mv"
      },
      "source": [
        "Important to understand here is that, because Spark’s execution is “lazy” execution, nothing has been executed yet. Your data hasn’t been actually read in. The rdd and header variables are actually just concepts in your mind. You have to push Spark to work for you, so let’s use the ```collect()``` method to look at the header:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6k7ny8LV4Mw",
        "outputId": "6487e044-6bdc-497c-c55c-8b1229a59b71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['longitude: continuous.',\n",
              " 'latitude: continuous.',\n",
              " 'housingMedianAge: continuous. ',\n",
              " 'totalRooms: continuous. ',\n",
              " 'totalBedrooms: continuous. ',\n",
              " 'population: continuous. ',\n",
              " 'households: continuous. ',\n",
              " 'medianIncome: continuous. ',\n",
              " 'medianHouseValue: continuous. ']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "header.collect()\n",
        "\n",
        "#The collect() method brings the entire RDD to a single machine, and you’ll get to see the following result:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReSSy75uV4Mx"
      },
      "source": [
        "Tip: be careful when using ```collect()```! Running this line of code can possibly cause the driver to run out of memory.\n",
        "That’s why the following approach with the take() method is a safer approach if you want to just print a few\n",
        "elements of the RDD. In general, it’s a good principle to limit your result set whenever possible,\n",
        "just like when you’re using SQL.\n",
        "\n",
        "You learn that the order of the variables is the same as the one that you saw above in the presentation of the data set, and you also learn that all columns should have continuous values. Let’s force Spark to do some more work and take a look at the California housing data to confirm this.\n",
        "\n",
        "Call the ```take()``` method on your RDD:\n",
        "\n",
        "By executing the ```take()``` method, you take the first 2 elements of the RDD. The result is as you expected: because you read in the files with the ```textFile()``` function, the lines are just all read in together. The entries are separated by a single comma and the rows themselves are also separated by a comma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aZXO_IrxV4Mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48347e02-0197-41aa-add1-d16f2b2d2ab8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000',\n",
              " '-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhucbU8GV4My"
      },
      "source": [
        "You definitely need to solve this. Now, you don’t need to split the entries, but you definitely need to make sure that the rows of your data are separate elements. To solve this, you’ll use the ```map()``` function to which you pass a lambda function to split the line at the comma. Then, check your result by running the same line with the ```take()``` method, just like you did before:\n",
        "\n",
        "Remember that `lambda` functions are anonymous functions which are created at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HZDwq94WV4My"
      },
      "outputs": [],
      "source": [
        "rdd = rdd.map(lambda line: line.split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Yg_HZo7IV4My",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677b15c5-e801-4b62-f05f-265c5f7c839f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['-122.230000',\n",
              "  '37.880000',\n",
              "  '41.000000',\n",
              "  '880.000000',\n",
              "  '129.000000',\n",
              "  '322.000000',\n",
              "  '126.000000',\n",
              "  '8.325200',\n",
              "  '452600.000000'],\n",
              " ['-122.220000',\n",
              "  '37.860000',\n",
              "  '21.000000',\n",
              "  '7099.000000',\n",
              "  '1106.000000',\n",
              "  '2401.000000',\n",
              "  '1138.000000',\n",
              "  '8.301400',\n",
              "  '358500.000000']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "rdd.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-exmBJxV4Mz"
      },
      "source": [
        "If you’re used to working with Pandas or data frames in R, you’ll have probably also expected to see a header, but there is none. To make your life easier, you will move on from the RDD and convert it to a DataFrame. Dataframes are preferred over RDDs whenever you can use them. Especially when you’re working with Python, the performance of DataFrames is better than RDDs.\n",
        "\n",
        "But what is the difference between the two?\n",
        "\n",
        "You can use RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don’t necessarily want the performance benefits that DataFrames can offer for (semi-)structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
        "\n",
        "To recapitulate, you’ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access.\n",
        "\n",
        "So let’s do this.\n",
        "\n",
        "The first step is to make a SchemaRDD or an RDD of Row objects with a schema. This is normal, because just like a DataFrame, you eventually want to come to a situation where you have rows and columns. Each entry is linked to a row and a certain column and columns have data types.\n",
        "\n",
        "You’ll use the `map()` function again and another lambda function in which you’ll map each entry to a field in a Row. to do this consider the first line:\n",
        "\n",
        "`[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000']`\n",
        "\n",
        "The lambda function says that you’re going to construct a row in a SchemaRDD and that the element at `index 0` will have the name “longitude”, and so on.\n",
        "\n",
        "With this SchemaRDD in place, you can easily convert the RDD to a DataFrame with the `toDF()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BXzXjVZDV4Mz"
      },
      "outputs": [],
      "source": [
        "df = rdd.map(lambda line: Row(longitude=line[0],\n",
        "                              latitude=line[1],\n",
        "                              housingMedianAge=line[2],\n",
        "                              totalRooms=line[3],\n",
        "                              totalBedRooms=line[4],\n",
        "                              population=line[5],\n",
        "                              households=line[6],\n",
        "                              medianIncome=line[7],\n",
        "                              medianHouseValue=line[8])).toDF()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC6gvhmBV4M0"
      },
      "source": [
        "Now that you have your DataFrame df, you can inspect it with the methods that you have also used before, namely `first()` and `take()`, but also with `head()` and `show()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GzlNNApgV4M1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f881a2b-fe42-4f65-e5fb-bd1888eb76ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
            "|  longitude| latitude|housingMedianAge| totalRooms|totalBedRooms| population| households|medianIncome|medianHouseValue|\n",
            "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
            "|-122.230000|37.880000|       41.000000| 880.000000|   129.000000| 322.000000| 126.000000|    8.325200|   452600.000000|\n",
            "|-122.220000|37.860000|       21.000000|7099.000000|  1106.000000|2401.000000|1138.000000|    8.301400|   358500.000000|\n",
            "|-122.240000|37.850000|       52.000000|1467.000000|   190.000000| 496.000000| 177.000000|    7.257400|   352100.000000|\n",
            "|-122.250000|37.850000|       52.000000|1274.000000|   235.000000| 558.000000| 219.000000|    5.643100|   341300.000000|\n",
            "|-122.250000|37.850000|       52.000000|1627.000000|   280.000000| 565.000000| 259.000000|    3.846200|   342200.000000|\n",
            "|-122.250000|37.850000|       52.000000| 919.000000|   213.000000| 413.000000| 193.000000|    4.036800|   269700.000000|\n",
            "|-122.250000|37.840000|       52.000000|2535.000000|   489.000000|1094.000000| 514.000000|    3.659100|   299200.000000|\n",
            "|-122.250000|37.840000|       52.000000|3104.000000|   687.000000|1157.000000| 647.000000|    3.120000|   241400.000000|\n",
            "|-122.260000|37.840000|       42.000000|2555.000000|   665.000000|1206.000000| 595.000000|    2.080400|   226700.000000|\n",
            "|-122.250000|37.840000|       52.000000|3549.000000|   707.000000|1551.000000| 714.000000|    3.691200|   261100.000000|\n",
            "|-122.260000|37.850000|       52.000000|2202.000000|   434.000000| 910.000000| 402.000000|    3.203100|   281500.000000|\n",
            "|-122.260000|37.850000|       52.000000|3503.000000|   752.000000|1504.000000| 734.000000|    3.270500|   241800.000000|\n",
            "|-122.260000|37.850000|       52.000000|2491.000000|   474.000000|1098.000000| 468.000000|    3.075000|   213500.000000|\n",
            "|-122.260000|37.840000|       52.000000| 696.000000|   191.000000| 345.000000| 174.000000|    2.673600|   191300.000000|\n",
            "|-122.260000|37.850000|       52.000000|2643.000000|   626.000000|1212.000000| 620.000000|    1.916700|   159200.000000|\n",
            "|-122.260000|37.850000|       50.000000|1120.000000|   283.000000| 697.000000| 264.000000|    2.125000|   140000.000000|\n",
            "|-122.270000|37.850000|       52.000000|1966.000000|   347.000000| 793.000000| 331.000000|    2.775000|   152500.000000|\n",
            "|-122.270000|37.850000|       52.000000|1228.000000|   293.000000| 648.000000| 303.000000|    2.120200|   155500.000000|\n",
            "|-122.260000|37.840000|       50.000000|2239.000000|   455.000000| 990.000000| 419.000000|    1.991100|   158700.000000|\n",
            "|-122.270000|37.840000|       52.000000|1503.000000|   298.000000| 690.000000| 275.000000|    2.603300|   162900.000000|\n",
            "+-----------+---------+----------------+-----------+-------------+-----------+-----------+------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHe4SM3KV4M1"
      },
      "source": [
        "The data seems all nicely ordered into columns, but what about the data types? By reading in your data, Spark will try to infer a schema, but has this been successful here? Use either `df.dtypes` or `df.printSchema()` to get to know more about the data types that are contained within your DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4PxTS1EZV4M1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db66274-8d65-456a-d81b-ae5bca76a661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housingMedianAge: string (nullable = true)\n",
            " |-- totalRooms: string (nullable = true)\n",
            " |-- totalBedRooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- medianIncome: string (nullable = true)\n",
            " |-- medianHouseValue: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em56Iz1xV4M1"
      },
      "source": [
        "All columns are still of data type string…\n",
        "\n",
        "If you want to continue with this DataFrame, you’ll need to rectify this situation and assign “better” or more accurate data types to all columns. Your performance will also benefit from this. Intuitively, you could go for a solution like the following, where you declare that each column of the DataFrame df should be cast to a `FloatType()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8AfEdUhAV4M2"
      },
      "outputs": [],
      "source": [
        "def convertColumn(df, names, newType):\n",
        "  for name in names:\n",
        "     df = df.withColumn(name, df[name].cast(newType))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SbjeG9FQV4M2"
      },
      "outputs": [],
      "source": [
        "# Assign all column names to `columns`\n",
        "columns = df.columns\n",
        "\n",
        "# Conver the `df` columns to `FloatType()`\n",
        "df = convertColumn(df, columns, FloatType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mCkGsgV4M2"
      },
      "source": [
        "Now that you’ve got that all sorted out, it’s time to really get started on the data exploration. You have seen that columnar access and SQL queries were two advantages of using DataFrames. Well, now it’s time to dig a little bit further into that. Let’s start small and just select two columns from df of which you only want to see 10 rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SDLtGp-WV4M2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9fa854-7807-4e68-afd5-ae67d6f745d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|population|totalBedRooms|\n",
            "+----------+-------------+\n",
            "|     322.0|        129.0|\n",
            "|    2401.0|       1106.0|\n",
            "|     496.0|        190.0|\n",
            "|     558.0|        235.0|\n",
            "|     565.0|        280.0|\n",
            "|     413.0|        213.0|\n",
            "|    1094.0|        489.0|\n",
            "|    1157.0|        687.0|\n",
            "|    1206.0|        665.0|\n",
            "|    1551.0|        707.0|\n",
            "+----------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('population','totalBedRooms').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8HTkVbYdV4M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010d9373-3981-497a-e966-c988d2dfa85d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: float (nullable = true)\n",
            " |-- latitude: float (nullable = true)\n",
            " |-- housingMedianAge: float (nullable = true)\n",
            " |-- totalRooms: float (nullable = true)\n",
            " |-- totalBedRooms: float (nullable = true)\n",
            " |-- population: float (nullable = true)\n",
            " |-- households: float (nullable = true)\n",
            " |-- medianIncome: float (nullable = true)\n",
            " |-- medianHouseValue: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrVu-OrnV4M3"
      },
      "source": [
        "A more complex example.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Bv4H-7DtV4M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161e10cf-63f5-42cf-fd2f-cdc7654a965a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+\n",
            "|housingMedianAge|count|\n",
            "+----------------+-----+\n",
            "|            52.0| 1273|\n",
            "|            51.0|   48|\n",
            "|            50.0|  136|\n",
            "|            49.0|  134|\n",
            "|            48.0|  177|\n",
            "|            47.0|  198|\n",
            "|            46.0|  245|\n",
            "|            45.0|  294|\n",
            "|            44.0|  356|\n",
            "|            43.0|  353|\n",
            "|            42.0|  368|\n",
            "|            41.0|  296|\n",
            "|            40.0|  304|\n",
            "|            39.0|  369|\n",
            "|            38.0|  394|\n",
            "|            37.0|  537|\n",
            "|            36.0|  862|\n",
            "|            35.0|  824|\n",
            "|            34.0|  689|\n",
            "|            33.0|  615|\n",
            "+----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8tcclmpV4M3"
      },
      "source": [
        "Besides querying, you can also choose to describe your data and get some summary statistics. This will most definitely help you after!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ACkl8xk1V4M3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439d569e-343e-42f0-a700-23574d980af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
            "|summary|       households|  housingMedianAge|  medianHouseValue|      medianIncome|        population|\n",
            "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
            "|  count|            20640|             20640|             20640|             20640|             20640|\n",
            "|   mean|499.5396802325581|28.639486434108527|206855.81690891474|3.8706710030346416|1425.4767441860465|\n",
            "| stddev|382.3297528316098| 12.58555761211163|115395.61587441359|1.8998217183639696|  1132.46212176534|\n",
            "|    min|              1.0|               1.0|           14999.0|            0.4999|               3.0|\n",
            "|    max|           6082.0|              52.0|          500001.0|           15.0001|           35682.0|\n",
            "+-------+-----------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df['households', 'housingMedianAge', 'medianHouseValue', 'medianIncome', 'population'].describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8CGyCKV4M4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2fpgIZFV4M4"
      },
      "source": [
        "You shouldn’t care about missing values; all zero values have been excluded from the data set.\n",
        "\n",
        "You should probably standardize your data, as you have seen that the range of minimum and maximum values is quite big.\n",
        "\n",
        "There are possibbly some additional attributes that you could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\n",
        "\n",
        "Your dependent variable is also quite big; To make your life easier, you’ll have to adjust the values slightly.\n",
        "\n",
        "Preprocessing The Target Values\n",
        "\n",
        "First, let’s start with the medianHouseValue, your dependent variable. To facilitate your working with the target values, you will express the house values in units of 100,000. That means that a target such as 452600.000000 should become 4.526:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CQkIFOqxV4M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d96cd3-150c-44c0-e945-a66738eee998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
            "|longitude|latitude|housingMedianAge|totalRooms|totalBedRooms|population|households|medianIncome|medianHouseValue|\n",
            "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
            "|  -122.23|   37.88|            41.0|     880.0|        129.0|     322.0|     126.0|      8.3252|           4.526|\n",
            "|  -122.22|   37.86|            21.0|    7099.0|       1106.0|    2401.0|    1138.0|      8.3014|           3.585|\n",
            "+---------+--------+----------------+----------+-------------+----------+----------+------------+----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Adjust the values of `medianHouseValue`\n",
        "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
        "\n",
        "# Show the first 2 lines of `df`\n",
        "df.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpS5mqciV4M5"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2OAGwcmV4M5"
      },
      "source": [
        "Now that you have adjusted the values in medianHouseValue, you can also add the additional variables that you read about above. You’re going to add the following columns to the data set:\n",
        "\n",
        "Rooms per household which refers to the number of rooms in households per block group;\n",
        "Population per household, which basically gives you an indication of how many people live in households per block group; And\n",
        "Bedrooms per room which will give you an idea about how many rooms are bedrooms per block group;\n",
        "\n",
        "As you’re working with DataFrames, you can best use the `select()` method to select the columns that you’re going to be working with, namely totalRooms, households, and population. Additionally, you have to indicate that you’re working with columns by adding the `col()` function to your code. Otherwise, you won’t be able to do element-wise operations like the division that you have in mind for these three variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8pJHIOwPV4M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f074be9-a8a4-41a6-b555-ec9926ceabea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Divide `totalRooms` by `households`\n",
        "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
        "\n",
        "# Divide `population` by `households`\n",
        "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
        "\n",
        "# Divide `totalBedRooms` by `totalRooms`\n",
        "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
        "\n",
        "# Add the new columns to `df`\n",
        "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
        "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
        "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
        "\n",
        "# Inspect the result\n",
        "df.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvuE-ZpqV4M5"
      },
      "source": [
        "Next, -and this is already forseeing an issue that you might have when you’ll standardize the values in your data set- you’ll also re-order the values. Since you don’t want to necessarily standardize your target values, you’ll want to make sure to isolate those in your data set.\n",
        "\n",
        "In this case, you’ll need to do this by using the `select()` method and passing the column names in the order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it won’t be affected by the standardization.\n",
        "\n",
        "Note also that this is the time to leave out variables that you might not want to consider in your analysis. In this case, let’s leave out variables such as longitude, latitude, housingMedianAge and totalRooms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1SrDJ_zhV4M5"
      },
      "outputs": [],
      "source": [
        "# Re-order and select columns\n",
        "df = df.select(\"medianHouseValue\",\n",
        "              \"totalBedRooms\",\n",
        "              \"population\",\n",
        "              \"households\",\n",
        "              \"medianIncome\",\n",
        "              \"roomsPerHousehold\",\n",
        "              \"populationPerHousehold\",\n",
        "              \"bedroomsPerRoom\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LFtP_ywV4M5"
      },
      "source": [
        "# Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRcTy965V4M5"
      },
      "source": [
        "Now that you have re-ordered the data, you’re ready to normalize the data. Or almost, at least. There is just one more step that you need to go through: separating the features from the target variable. In essence, this boils down to isolating the first column in your DataFrame from the rest of the columns.\n",
        "\n",
        "In this case, you’ll use the `map()` function that you use with RDDs to perform this action. You also see that you make use of the `DenseVector()` function. A dense vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\n",
        "\n",
        "Next, you go back to making a DataFrame out of the input_data and you re-label the columns by passing a list as a second argument. This list consists of the column names \"label\" and \"features\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cyVm4dxSV4M5"
      },
      "outputs": [],
      "source": [
        "# Define the `input_data`\n",
        "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
        "\n",
        "# Replace `df` with the new DataFrame\n",
        "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8N35Qq4V4M6"
      },
      "source": [
        "Next, you can finally scale the data. You can use Spark ML to do this: this library will make machine learning on big data scalable and easy. You’ll find tools such as ML algorithms and everything you need to build practical ML pipelines. In this case, you don’t need to do that much preprocessing so a pipeline would maybe be overkill, but if you want to look into it, definitely consider visiting this [page](https://spark.apache.org/docs/latest/ml-pipeline.html)\n",
        "\n",
        "The input columns are the features, and the output column with the rescaled that will be included in the `scaled_df` will be named \"features_scaled\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YmJFqAKcV4M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee76f4a2-ca25-419a-b0c9-2eeaed80a3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            features|     features_scaled|\n",
            "+-----+--------------------+--------------------+\n",
            "|4.526|[129.0,322.0,126....|[0.30623297630686...|\n",
            "|3.585|[1106.0,2401.0,11...|[2.62553233949916...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the `standardScaler`\n",
        "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "# Fit the DataFrame to the scaler\n",
        "scaler = standardScaler.fit(df)\n",
        "\n",
        "# Transform the data in `df` with the scaler\n",
        "scaled_df = scaler.transform(df)\n",
        "\n",
        "# Inspect the result\n",
        "scaled_df.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "id": "8leo38LiV4M7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}